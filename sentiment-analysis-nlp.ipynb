{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10172688,"sourceType":"datasetVersion","datasetId":6282898}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T19:51:04.012224Z","iopub.execute_input":"2025-05-19T19:51:04.012595Z","iopub.status.idle":"2025-05-19T19:51:05.678299Z","shell.execute_reply.started":"2025-05-19T19:51:04.012558Z","shell.execute_reply":"2025-05-19T19:51:05.677128Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment-and-emotion-analysis-dataset/archive/combined_emotion.csv\n/kaggle/input/sentiment-and-emotion-analysis-dataset/archive/combined_sentiment_data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import nltk \nimport re\nimport pandas as pd\ndf1=pd.read_csv('/kaggle/input/sentiment-and-emotion-analysis-dataset/archive/combined_emotion.csv')\ndf2=pd.read_csv('/kaggle/input/sentiment-and-emotion-analysis-dataset/archive/combined_sentiment_data.csv')\ndf2.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T19:51:05.680307Z","iopub.execute_input":"2025-05-19T19:51:05.680729Z","iopub.status.idle":"2025-05-19T19:51:09.996517Z","shell.execute_reply.started":"2025-05-19T19:51:05.680705Z","shell.execute_reply":"2025-05-19T19:51:09.995305Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                            sentence sentiment\n0  So there is no way for me to plug it in here i...  negative\n1                        Good case, Excellent value.  positive\n2                             Great for the jawbone.  positive\n3  Tied to charger for conversations lasting more...  negative\n4                                  The mic is great.  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>So there is no way for me to plug it in here i...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Good case, Excellent value.</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Great for the jawbone.</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Tied to charger for conversations lasting more...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The mic is great.</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df1['Cleaned_Text'] = df1['sentence'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\ndf2['Cleaned_Text']=df2['sentence'].str.replace(r'[^a-zA-Z0-9\\s]','',regex=True)\n\ndf1['Cleaned_Text'] = df1['Cleaned_Text'].str.replace(r'\\s+', ' ', regex=True).str.strip().str.lower()\ndf2['Cleaned_Text']=df2['Cleaned_Text'].str.replace(r'\\s+', ' ', regex=True).str.strip().str.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T19:51:09.997518Z","iopub.execute_input":"2025-05-19T19:51:09.997904Z","iopub.status.idle":"2025-05-19T19:51:14.052309Z","shell.execute_reply.started":"2025-05-19T19:51:09.997871Z","shell.execute_reply":"2025-05-19T19:51:14.051394Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(f'df1',df1.shape)\nprint(f'df2',df2.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T19:51:14.053312Z","iopub.execute_input":"2025-05-19T19:51:14.053667Z","iopub.status.idle":"2025-05-19T19:51:14.059699Z","shell.execute_reply.started":"2025-05-19T19:51:14.053638Z","shell.execute_reply":"2025-05-19T19:51:14.058567Z"}},"outputs":[{"name":"stdout","text":"df1 (422746, 3)\ndf2 (3309, 3)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!python -m spacy download en_core_web_sm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T19:51:14.062088Z","iopub.execute_input":"2025-05-19T19:51:14.062410Z","iopub.status.idle":"2025-05-19T19:51:32.639366Z","shell.execute_reply.started":"2025-05-19T19:51:14.062386Z","shell.execute_reply":"2025-05-19T19:51:32.637849Z"}},"outputs":[{"name":"stdout","text":"Collecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T19:51:32.640864Z","iopub.execute_input":"2025-05-19T19:51:32.641253Z","iopub.status.idle":"2025-05-19T19:51:32.654560Z","shell.execute_reply.started":"2025-05-19T19:51:32.641179Z","shell.execute_reply":"2025-05-19T19:51:32.653423Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                            sentence emotion  \\\n0      i just feel really helpless and heavy hearted    fear   \n1  ive enjoyed being able to slouch about relax a...     sad   \n2  i gave up my internship with the dmrg and am f...    fear   \n3                         i dont know i feel so lost     sad   \n4  i am a kindergarten teacher and i am thoroughl...    fear   \n\n                                        Cleaned_Text  \n0      i just feel really helpless and heavy hearted  \n1  ive enjoyed being able to slouch about relax a...  \n2  i gave up my internship with the dmrg and am f...  \n3                         i dont know i feel so lost  \n4  i am a kindergarten teacher and i am thoroughl...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>emotion</th>\n      <th>Cleaned_Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i just feel really helpless and heavy hearted</td>\n      <td>fear</td>\n      <td>i just feel really helpless and heavy hearted</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ive enjoyed being able to slouch about relax a...</td>\n      <td>sad</td>\n      <td>ive enjoyed being able to slouch about relax a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i gave up my internship with the dmrg and am f...</td>\n      <td>fear</td>\n      <td>i gave up my internship with the dmrg and am f...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i dont know i feel so lost</td>\n      <td>sad</td>\n      <td>i dont know i feel so lost</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i am a kindergarten teacher and i am thoroughl...</td>\n      <td>fear</td>\n      <td>i am a kindergarten teacher and i am thoroughl...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from tqdm import tqdm\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\ntqdm.pandas()\n\n# Function for batch lemmatization\ndef spacy_batch_lemmatizer(texts):\n    docs = nlp.pipe(texts, batch_size=1000)\n    return [' '.join([token.lemma_ for token in doc]) for doc in tqdm(docs, total=len(texts))]\n\n# Apply to your cleaned text column\ndf1['Lemmatized_Text'] = spacy_batch_lemmatizer(df1['Cleaned_Text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T19:51:32.655705Z","iopub.execute_input":"2025-05-19T19:51:32.656237Z","iopub.status.idle":"2025-05-19T20:01:25.329938Z","shell.execute_reply.started":"2025-05-19T19:51:32.656202Z","shell.execute_reply":"2025-05-19T20:01:25.328930Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 422746/422746 [09:47<00:00, 719.75it/s] \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df2['Lemmatized_Text'] = spacy_batch_lemmatizer(df2['Cleaned_Text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T20:01:25.331068Z","iopub.execute_input":"2025-05-19T20:01:25.331749Z","iopub.status.idle":"2025-05-19T20:01:28.619403Z","shell.execute_reply.started":"2025-05-19T20:01:25.331712Z","shell.execute_reply":"2025-05-19T20:01:28.618501Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 3309/3309 [00:03<00:00, 1011.58it/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"df2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T20:01:28.620367Z","iopub.execute_input":"2025-05-19T20:01:28.620637Z","iopub.status.idle":"2025-05-19T20:01:28.631984Z","shell.execute_reply.started":"2025-05-19T20:01:28.620610Z","shell.execute_reply":"2025-05-19T20:01:28.630824Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                            sentence sentiment  \\\n0  So there is no way for me to plug it in here i...  negative   \n1                        Good case, Excellent value.  positive   \n2                             Great for the jawbone.  positive   \n3  Tied to charger for conversations lasting more...  negative   \n4                                  The mic is great.  positive   \n\n                                        Cleaned_Text  \\\n0  so there is no way for me to plug it in here i...   \n1                          good case excellent value   \n2                              great for the jawbone   \n3  tied to charger for conversations lasting more...   \n4                                   the mic is great   \n\n                                     Lemmatized_Text  \n0  so there be no way for I to plug it in here in...  \n1                          good case excellent value  \n2                              great for the jawbone  \n3  tie to charger for conversation last more than...  \n4                                   the mic be great  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>sentiment</th>\n      <th>Cleaned_Text</th>\n      <th>Lemmatized_Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>So there is no way for me to plug it in here i...</td>\n      <td>negative</td>\n      <td>so there is no way for me to plug it in here i...</td>\n      <td>so there be no way for I to plug it in here in...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Good case, Excellent value.</td>\n      <td>positive</td>\n      <td>good case excellent value</td>\n      <td>good case excellent value</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Great for the jawbone.</td>\n      <td>positive</td>\n      <td>great for the jawbone</td>\n      <td>great for the jawbone</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Tied to charger for conversations lasting more...</td>\n      <td>negative</td>\n      <td>tied to charger for conversations lasting more...</td>\n      <td>tie to charger for conversation last more than...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The mic is great.</td>\n      <td>positive</td>\n      <td>the mic is great</td>\n      <td>the mic be great</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntexts1 = df1['Lemmatized_Text']\nvectorizer1 = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), stop_words='english')\nX1_tfidf = vectorizer1.fit_transform(texts1)\n\ntexts2 = df2['Lemmatized_Text']\nvectorizer2 = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), stop_words='english')\nX2_tfidf = vectorizer2.fit_transform(texts2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T20:01:28.633052Z","iopub.execute_input":"2025-05-19T20:01:28.633394Z","iopub.status.idle":"2025-05-19T20:01:44.187113Z","shell.execute_reply.started":"2025-05-19T20:01:28.633360Z","shell.execute_reply":"2025-05-19T20:01:44.185103Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X1_tfidf, df1['emotion'], test_size=0.2, random_state=42)\n\n# Train model\nmodel1 = MultinomialNB()\nmodel1.fit(X_train1, y_train1)\n\n# Evaluate\ny_pred1 = model1.predict(X_test1)\nprint(\"Emotion Classification Accuracy:\", accuracy_score(y_test1, y_pred1))\nprint(classification_report(y_test1, y_pred1))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T20:01:44.188114Z","iopub.execute_input":"2025-05-19T20:01:44.188567Z","iopub.status.idle":"2025-05-19T20:01:48.165699Z","shell.execute_reply.started":"2025-05-19T20:01:44.188533Z","shell.execute_reply":"2025-05-19T20:01:48.164767Z"}},"outputs":[{"name":"stdout","text":"Emotion Classification Accuracy: 0.8731519810762862\n              precision    recall  f1-score   support\n\n       anger       0.94      0.85      0.89     11810\n        fear       0.88      0.81      0.85      9952\n         joy       0.84      0.96      0.89     28781\n        love       0.87      0.58      0.70      6929\n         sad       0.89      0.95      0.92     24036\n     suprise       0.88      0.48      0.63      3042\n\n    accuracy                           0.87     84550\n   macro avg       0.88      0.77      0.81     84550\nweighted avg       0.88      0.87      0.87     84550\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"X_train2, X_test2, y_train2, y_test2 = train_test_split(X2_tfidf, df2['sentiment'], test_size=0.2, random_state=42)\n\nmodel2 = MultinomialNB()\nmodel2.fit(X_train2, y_train2)\n\n# Evaluate\ny_pred2 = model2.predict(X_test2)\nprint(\"Sentiment Classification Accuracy:\", accuracy_score(y_test2, y_pred2))\nprint(classification_report(y_test2, y_pred2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T20:01:48.166766Z","iopub.execute_input":"2025-05-19T20:01:48.167092Z","iopub.status.idle":"2025-05-19T20:01:48.211803Z","shell.execute_reply.started":"2025-05-19T20:01:48.167062Z","shell.execute_reply":"2025-05-19T20:01:48.211020Z"}},"outputs":[{"name":"stdout","text":"Sentiment Classification Accuracy: 0.8006042296072508\n              precision    recall  f1-score   support\n\n    negative       0.83      0.77      0.80       334\n    positive       0.78      0.84      0.81       328\n\n    accuracy                           0.80       662\n   macro avg       0.80      0.80      0.80       662\nweighted avg       0.80      0.80      0.80       662\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"df1.to_csv('combined_emotion_cleaned.csv', index=False)\n\ndf2.to_csv('combined_sentiment_cleaned.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T20:01:48.212673Z","iopub.execute_input":"2025-05-19T20:01:48.212910Z","iopub.status.idle":"2025-05-19T20:01:52.794565Z","shell.execute_reply.started":"2025-05-19T20:01:48.212891Z","shell.execute_reply":"2025-05-19T20:01:52.793706Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\nresults_df1 = pd.DataFrame({\n    'Actual': y_test1.values,\n    'Predicted': y_pred1\n})\n\nresults_df1['Text'] = df1.loc[y_test1.index, 'Lemmatized_Text'].values\n\nresults_df1.to_csv('emotion_test_predictions.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T20:01:52.797631Z","iopub.execute_input":"2025-05-19T20:01:52.797976Z","iopub.status.idle":"2025-05-19T20:01:53.221296Z","shell.execute_reply.started":"2025-05-19T20:01:52.797954Z","shell.execute_reply":"2025-05-19T20:01:53.220160Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"results_df2 = pd.DataFrame({\n    'Actual': y_test2.values,\n    'Predicted': y_pred2\n})\n\n# Optionally include the original text\nresults_df2['Text'] = df2.loc[y_test2.index, 'Lemmatized_Text'].values\n\n# Save to CSV\nresults_df2.to_csv('sentiment_test_predictions.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T20:01:53.222485Z","iopub.execute_input":"2025-05-19T20:01:53.223111Z","iopub.status.idle":"2025-05-19T20:01:53.235719Z","shell.execute_reply.started":"2025-05-19T20:01:53.223075Z","shell.execute_reply":"2025-05-19T20:01:53.234692Z"}},"outputs":[],"execution_count":15}]}